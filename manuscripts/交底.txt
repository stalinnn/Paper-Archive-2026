本发明公开了一种基于自适应稀疏门控的序列处理与优化方法、系统及介质，涉及人工智能与深度学习技术领域。该方法包括：执行惯性演化，利用惯性处理单元以线性复杂度常驻运行并更新隐状态；进行熵判决，计算反映预测不确定性的信息熵并生成门控系数；执行按需激活，根据门控系数在平庸时刻物理旁路几何修正单元，在关键时刻激活该单元并加载历史键值对进行全局自注意力计算；最后基于门控系数进行流形融合，生成输出状态。本发明通过自适应稀疏计算机制，在大幅降低推理成本的同时，有效保证了模型在复杂任务中的高精度与抗遗忘能力。





1. 一种基于自适应稀疏门控的序列处理与优化方法，其特征在于，应用于热力学门控网络架构，所述架构包含并行的惯性处理单元和几何修正单元；所述方法包括以下步骤：
步骤S1、惯性演化：接收序列数据的当前时刻输入，将其传入所述惯性处理单元进行计算，更新惯性隐状态；其中，所述惯性处理单元维持低能耗运行，以线性计算复杂度处理序列的局部依赖；
步骤S2、熵判决：利用门控判决单元对所述惯性隐状态进行评估，计算反映当前时刻预测不确定性的指标，并根据所述指标生成门控系数；所述门控判决的粒度包括单个时间步（Token-level）或预设长度的时间块（Chunk-level）；
步骤S3、按需激活：根据所述门控系数判断当前时刻属于平庸时刻还是关键时刻；若判定为平庸时刻，则控制所述几何修正单元处于物理旁路状态，跳过二次复杂度的矩阵运算，不分配新的键值对缓存；若判定为关键时刻，则激活所述几何修正单元，读取历史键值对缓存进行全局检索与自注意力计算，生成几何修正向量；
步骤S4、流形融合：基于所述门控系数，对所述惯性隐状态和所述几何修正向量进行加权融合，生成当前时刻的输出状态。
2. 如权利要求1所述的方法，其特征在于，所述惯性处理单元采用循环神经网络、门控循环单元或状态空间模型中的任意一种；所述惯性演化步骤在序列处理的每一时刻始终执行，利用所述惯性隐状态作为兜底记忆以维持对序列环境的连续感知。
3. 如权利要求1所述的方法，其特征在于，所述门控判决单元作为系统的自适应门控，所述生成门控系数的步骤具体包括：
计算所述惯性隐状态的即时信息熵、几何秩、预测误差（Loss）、隐状态范数或通过神经网络学习得到的置信度分数；当所述指标低于预设阈值时，判定模型对当前预测具有高置信度，将所述门控系数置为0，对应所述平庸时刻；当所述指标高于预设阈值时，判定模型处于高惊奇度状态，将所述门控系数置为非0值，对应所述关键时刻。
4. 如权利要求1所述的方法，其特征在于，所述几何修正单元采用点积自注意力机制；所述激活所述几何修正单元的步骤具体包括：通过直接内存访问请求加载存储于冷存储中的历史键值对缓存；计算当前查询向量与历史键值对缓存之间的注意力得分；根据所述注意力得分对历史值向量进行聚合，得到所述几何修正向量，以捕捉序列的长程依赖。
5. 如权利要求1所述的方法，其特征在于，所述步骤S4中的流形融合遵循以下热力学融合公式：
Hout=(1−gt)×Hrnn+gt×Hattn
其中，Hout为输出状态，gt为所述门控系数（gt∈[0,1]），Hrnn为所述惯性隐状态，Hattn为所述几何修正向量；当gt≈0时，系统仅输出惯性模态特征；当gt>0时，系统引入几何模态特征对惯性模态特征进行修正。
6. 如权利要求1所述的方法，其特征在于，还包括基于热力学损失函数的模型训练步骤：构建包含任务预测损失与稀疏性惩罚损失的复合目标函数；
Ltotal=Ltask(θ,ϕ)+λ⋅Lsparsity(ϕ)
其中，Ltask为预测误差，Lsparsity为门控系数的L1范数，λ为热力学正则化系数；在训练过程中，利用反向传播算法最小化所述复合目标函数，迫使所述门控判决单元自发学习在低惊奇度时刻关闭、在高惊奇度时刻开启的稀疏策略。
7. 如权利要求1所述的方法，其特征在于，还包括基于惯性兜底的动态缓存管理步骤：将所述惯性隐状态作为L1缓存常驻于高速计算内存中，用于维持物体恒常性与服务的连续性；将所述几何修正单元所需的历史键值对作为L2缓存置于冷存储中；仅在所述门控判决单元触发所述关键时刻时，才动态加载所述L2缓存；在所述几何修正单元处于物理旁路状态时，仅依赖所述L1缓存进行推理。
8. 如权利要求1所述的方法，其特征在于，还包括针对持续学习场景的结构化抗遗忘训练步骤：在针对新知识领域进行增量训练时，冻结所述惯性处理单元的模型参数，保留通用逻辑基座；利用新领域数据仅微调所述几何修正单元及所述门控判决单元的参数，通过更新几何通道承载特定事实知识，实现结构化增量学习。
9. 一种基于自适应稀疏门控的序列处理系统，其特征在于，采用热力学门控网络（TGN）架构，包括：
惯性处理模块，配置为常驻运行的线性通道，用于接收输入并更新固定大小的惯性隐状态，处理序列的语法结构及低熵信息；所述惯性处理模块可部署于低功耗的常开（Always-on）NPU子系统上；
门控判决模块，连接至所述惯性处理模块，配置为基于所述惯性隐状态的特征（包括熵、秩或学习到的置信度）检测当前输入的惊奇度，输出门控系数；
几何修正模块，配置为按需激活的非线性通道，用于执行全局自注意力计算；该模块受所述门控系数控制，仅在惊奇度超过阈值时启动以处理长程依赖及高熵信息，其余时间处于休眠状态；所述几何修正模块可部署于高性能GPU、存内计算单元或稀疏张量核上；
自适应融合模块，用于根据所述门控系数对惯性处理模块和几何修正模块的输出进行加权求和。
9. 一种电子设备，包括存储器、处理器及存储在存储器上并可在处理器上运行的计算机程序，其特征在于，所述处理器执行所述程序时实现如权利要求1至7中任一项所述的方法。
10. 一种计算机可读存储介质，其上存储有计算机程序，其特征在于，所述程序被处理器执行时实现如权利要求1至7中任一项所述的方法。


基于自适应稀疏门控的序列处理与优化方法、系统及介质
技术领域
本发明涉及人工智能与深度学习技术领域，尤其涉及一种基于自适应稀疏门控的序列处理与优化方法、系统及介质。
背景技术
随着深度学习技术的飞速发展，处理自然语言文本、基因组序列以及机器人传感器流等长序列数据已成为人工智能领域的核心任务。目前，主流的序列处理架构主要基于Transformer模型（如GPT、Llama等）。Transformer架构的核心在于自注意力机制，它允许序列中的每一个元素直接与所有其他元素进行交互。这种“全连接拓扑”结构赋予了模型强大的全局依赖捕捉能力。然而，标准Transformer模型在推理阶段面临严峻的挑战：
计算复杂度与显存瓶颈：自注意力机制的计算复杂度和显存占用随序列长度呈平方级（O(N2)）增长。在处理长上下文时，不仅推理成本极高，而且实时维护的键值缓存（KV Cache）会占用巨大的显存资源，导致在端侧设备上难以部署。
计算冗余：研究表明，在序列处理中并非所有位置都需要全局交互，强制对大量局部依赖（如语法停顿）进行全注意力计算造成了算力的巨大浪费 7。为了解决上述问题，现有技术中出现了两种主要的改进方向：
第一种方向是线性注意力变体或状态空间模型（SSM），以Mamba模型为代表。该类模型主要由线性时不变系统（LTI）构成，利用固定的隐藏状态（Hidden State,ht）以递归方式传递历史信息，实现了O(N)的线性和内存复杂度。然而，该类技术的缺点在于：存在“固定状态容量”瓶颈。由于试图将无限长的历史信息压缩进固定大小的隐状态向量中，当历史信息的熵超过容量上限时，必然发生有损压缩，导致在处理复杂联想回忆或极端长程依赖任务时出现“容量崩塌”现象，无法精准回忆起早期的关键细节。
第二种方向是稀疏注意力优化技术，以H2O（Heavy Hitter Oracle）算法为代表。该技术在推理过程中根据注意力得分动态丢弃低分Token的KV值，仅保留“重度命中者”。然而，该类技术的缺点在于：灾难性遗忘风险：这是一种“后验丢弃”机制，一旦某个Token被判定为暂时不重要而被丢弃，若后续时刻（如长文结尾呼应开头）需要再次用到它时，由于缓存已被清空且缺乏兜底记忆机制，模型无法找回该信息。硬件效率低：产生的稀疏模式往往是非结构化的，导致内存地址不连续，极大地降低了GPU/NPU的内存读取效率，难以实现实际推理速度的提升。
综上所述，现有技术在“长程记忆能力”、“推理计算效率”和“硬件友好性”三者之间存在难以调和的矛盾。如何设计一种既能像RNN一样低成本推理，又能像Transformer一样具备无损长程记忆，且避免灾难性遗忘的序列处理架构，是当前亟待解决的技术问题。
发明内容
本发明意在提供一种基于自适应稀疏门控的序列处理与优化方法、系统及介质，以解决现有技术中存在的不足，本发明要解决的技术问题通过以下技术方案来实现。
本发明第一方面提供一种基于自适应稀疏门控的序列处理与优化方法，其特征在于，应用于热力学门控网络架构，所述架构包含并行的惯性处理单元和几何修正单元；所述方法包括以下步骤：
步骤S1、惯性演化：接收序列数据的当前时刻输入，将其传入所述惯性处理单元进行计算，更新惯性隐状态；其中，所述惯性处理单元维持低能耗运行，以线性计算复杂度处理序列的局部依赖；
步骤S2、熵判决：利用门控判决单元对所述惯性隐状态进行评估，计算反映当前时刻预测不确定性的信息熵，并根据所述信息熵生成门控系数；
步骤S3、按需激活：根据所述门控系数判断当前时刻属于平庸时刻还是关键时刻；若判定为平庸时刻，则控制所述几何修正单元处于物理旁路状态，跳过二次复杂度的矩阵运算，不分配新的键值对缓存；若判定为关键时刻，则激活所述几何修正单元，读取历史键值对缓存进行全局检索与自注意力计算，生成几何修正向量；
步骤S4、流形融合：基于所述门控系数，对所述惯性隐状态和所述几何修正向量进行加权融合，生成当前时刻的输出状态。
在一个优选的实施例中，所述惯性处理单元采用循环神经网络、门控循环单元或状态空间模型中的任意一种；所述惯性演化步骤在序列处理的每一时刻始终执行，利用所述惯性隐状态作为兜底记忆以维持对序列环境的连续感知。
在一个优选的实施例中，所述门控判决单元作为系统的自适应门控，所述生成门控系数的步骤具体包括：
计算所述惯性隐状态的即时信息熵或几何秩；当所述信息熵或几何秩低于预设阈值时，判定模型对当前预测具有高置信度，将所述门控系数置为0，对应所述平庸时刻；当所述信息熵或几何秩高于预设阈值时，判定模型处于高惊奇度状态，将所述门控系数置为非0值，对应所述关键时刻。
在一个优选的实施例中，所述几何修正单元采用点积自注意力机制；所述激活所述几何修正单元的步骤具体包括：通过直接内存访问请求加载存储于冷存储中的历史键值对缓存；计算当前查询向量与历史键值对缓存之间的注意力得分；根据所述注意力得分对历史值向量进行聚合，得到所述几何修正向量，以捕捉序列的长程依赖。
在一个优选的实施例中，所述步骤S4中的流形融合遵循以下热力学融合公式：
Hout=(1−gt)×Hrnn+gt×Hattn
其中，Hout为输出状态，gt为所述门控系数（gt∈[0,1]），Hrnn为所述惯性隐状态，Hattn为所述几何修正向量；当gt≈0时，系统仅输出惯性模态特征；当gt>0时，系统引入几何模态特征对惯性模态特征进行修正。
在一个优选的实施例中，还包括基于惯性兜底的动态缓存管理步骤：将所述惯性隐状态作为L1缓存常驻于高速计算内存中，用于维持物体恒常性与服务的连续性；将所述几何修正单元所需的历史键值对作为L2缓存置于冷存储中；仅在所述门控判决单元触发所述关键时刻时，才动态加载所述L2缓存；在所述几何修正单元处于物理旁路状态时，仅依赖所述L1缓存进行推理。
在一个优选的实施例中，还包括针对持续学习场景的结构化抗遗忘训练步骤：在针对新知识领域进行增量训练时，冻结所述惯性处理单元的模型参数，保留通用逻辑基座；利用新领域数据仅微调所述几何修正单元及所述门控判决单元的参数，通过更新几何通道承载特定事实知识，实现结构化增量学习。
本发明第二方面提供了一种基于自适应稀疏门控的序列处理系统，采用热力学门控网络（TGN）架构，包括：惯性处理模块，配置为常驻运行的线性通道，用于接收输入并更新固定大小的惯性隐状态，处理序列的语法结构及低熵信息；门控判决模块，连接至所述惯性处理模块，配置为基于所述惯性隐状态的熵检测当前输入的惊奇度，输出门控系数；几何修正模块，配置为按需激活的非线性通道，用于执行全局自注意力计算；该模块受所述门控系数控制，仅在惊奇度超过阈值时启动以处理长程依赖及高熵信息，其余时间处于休眠状态；自适应融合模块，用于根据所述门控系数对惯性处理模块和几何修正模块的输出进行加权求和。
本发明第三方面提供了一种电子设备，包括存储器、处理器及存储在存储器上并可在处理器上运行的计算机程序，所述处理器执行所述程序时实现任一项所述的方法。
本发明的第四方面提供了一种计算机可读存储介质，其上存储有计算机程序，其特征在于，所述程序被处理器执行时实现上述任一项所述的方法。
与现有技术相比，本发明具有以下显著的有益效果：
1.突破了长序列建模中“计算效率”与“记忆能力”的矛盾，实现了推理成本的显著降低。本发明通过引入基于信息熵的门控判决单元（麦克斯韦妖），能够精准识别序列中的“平庸时刻”与“关键时刻”。在绝大多数低熵的平庸时刻（例如占比90%以上），系统自动物理旁路高能耗的几何修正单元，仅使用线性复杂度的惯性处理单元（RNN）运行。这使得推理阶段的平均计算复杂度从标准Transformer的 O(N2) 降低至接近O(N) ，大幅减少了矩阵乘法运算量，显著提升了推理吞吐量。
2. 解决了纯线性模型（RNN/SSM）的“记忆容量崩塌”问题，保证了复杂任务的高精度。针对Mamba等纯线性模型试图将无限历史压缩进固定隐状态而导致的细节丢失问题，本发明保留了并在关键时刻激活几何修正单元。当门控检测到预测误差或不确定性超过阈值时，系统能够瞬间开启全注意力机制，对原始历史数据进行全局检索。这种机制赋予了模型“无限长程记忆”的能力，确保了在多跳问答、复杂联想回忆等高难度任务中，模型依然能够保持与全注意力模型相当的高准确率。
3. 实现了“无损稀疏”与硬件友好的缓存管理，避免了灾难性遗忘。不同于现有稀疏算法（如H2O）采用的“后验丢弃”机制（容易导致永久性信息丢失），本发明采用了“先验判决”与“分级缓存”策略。惯性处理单元的隐状态作为“L1缓存”常驻内存，始终维护对环境的连续建模（物体恒常性），即使在几何单元关闭时也能提供兜底记忆。此外，本发明的稀疏性属于“时间分块”的结构化稀疏，保持了矩阵运算的连续性，相比于非结构化稀疏，更适配GPU/NPU的硬件架构，有效避免了内存带宽浪费。
4. 支持结构化的持续学习，延长了模型的生命周期。本发明的双通道异构架构为增量学习提供了天然的结构基础。在学习新领域的知识时，可以冻结承载通用逻辑与语法的惯性处理单元，仅微调承载特定事实知识的几何修正单元 9。这种“结构化增量学习”方式避免了新知识对旧逻辑基座的破坏，有效解决了深度学习模型在持续更新中常见的灾难性遗忘问题。
附图说明
图1是本发明的自适应稀疏门控的序列处理与优化方法步骤流程图；
图2是本发明自适应稀疏门控的序列处理系统模块图；
图3是本发明的电子设备实施例的结构示意图；
具体实施方式
为了使本技术领域的人员更好地理解本发明方案，下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述。显然，所描述的实施例仅仅是本发明一部分的实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都应当属于本发明保护的范围。
本发明实施例首先提供了一种基于自适应稀疏门控的序列处理与优化方法。如图1所示，该方法主要针对长度为T的输入序列X={x1,x2,...,xT}，在每一个时间步t依次执行以下处理步骤：
首先执行步骤S1、惯性演化。在时刻t，系统接收输入向量xt。无论当前的上下文环境如何，系统首先利用具有线性计算复杂度的惯性处理模块执行线性演化。在本实施例中，惯性处理模块采用状态空间模型。其离散化后的状态更新公式如下：
hrnnt=Ahrnnt−1+Bxt
其中，A和B是经过离散化参数变换后的系统矩阵，hrnnt−1是上一时刻的惯性隐状态。此步骤模拟了物体在物理环境中的惯性运动，由于仅涉及矩阵向量乘法，其计算复杂度为线性的O(1)（针对单步）。此步骤的技术效果在于确保模型具有“物体恒常性”，即即使后续的注意力机制关闭，模型依然通过hrnn维持着对环境的基础认知（如语法连贯性），防止上下文丢失。
随后执行步骤S2、熵判决与门控生成。在更新得到当前惯性隐状态hrnnt后，系统需要判断仅凭当前的惯性（RNN记忆）是否足以准确预测下一个Token。该步骤通过门控判决模块执行，支持两种粒度模式：
（1）逐词粒度（Token-level）：对每个时间步t，计算其预测分布的香农熵H(pt)；
（2）分块粒度（Chunk-level）：将输入序列划分为长度为L（例如128）的时间块，计算该块内所有隐状态的平均熵或最大熵作为该块的统一惊奇度指标。分块模式能显著减少门控切换频率，更利于硬件并行计算。
最后，将计算得到的指标映射到[0, 1]区间，生成门控系数gt。生成公式为：
gt=σ(α⋅(H(pt)−τ))，
其中σ为Sigmoid函数，τ为预设的熵阈值。当H(pt)<τ时，gt→0，判定为“平庸时刻”；当H(pt)>τ时，gt→1，判定为“关键时刻”。

接着执行步骤S3、按需激活与几何修正。系统根据门控系数gt的值决定执行路径。情形一：当门控系数低于极小值ϵ（例如0.05）时，判定为平庸时刻。此时，具有二次计算复杂度的几何修正模块被物理旁路。系统不执行任何自注意力计算，不加载历史KV Cache，也不分配新的显存，修正向量hattnt直接被置为零向量。此模式下，模型的推理速度极快，功耗极低。
情形二：当门控系数高于阈值时，判定为关键时刻。系统执行全量激活，具体操作包括：通过DMA（直接内存访问）控制器从冷存储（L2 Cache）中检索历史时刻的键值对(K1:t−1,V1:t−1)；随后执行标准的自注意力运算，计算查询向量Qt与历史键值对的注意力得分，并据此聚合历史信息得到修正向量hattnt。这相当于在序列流形上构建了连接历史与当前的“捷径”，突破了RNN的遗忘瓶颈。

最后执行步骤S4、流形融合。系统将惯性流与几何流进行融合，生成最终的输出状态Houtt。融合公式遵循热力学加权原则：
Houtt=(1−gt)⋅hrnnt+gt⋅hattnt。
该输出状态随后用于最终的预测及下一时刻的输入。

此外，为了获得上述自适应稀疏能力，本发明还包含一种基于热力学损失函数的训练方法。该训练方法构建了一个复合目标函数：
Ltotal=Ltask(θ,ϕ)+λ⋅1T∑t=1T|gt|
其中，Ltask是常规的交叉熵预测损失，代表系统的“内能”；第二项是门控系数的L1范数，代表系统的“计算功耗”或“熵”；λ是热力学正则化系数（计算温度）。在反向传播训练中，这两项损失形成博弈：
*   对于简单样本，Ltask很小，为了最小化总损失，模型会倾向于将gt压低至0（省电）；
*   对于困难样本，Ltask很大，模型被迫增大gt以开启Attention来降低预测误差，即使这会增加第二项的惩罚。
最终，模型自发收敛到一个“该省则省，该用则用”的自组织临界状态。针对持续学习场景，采用结构化更新策略：冻结承载通用语法的惯性处理模块参数，仅微调几何修正模块和门控判决模块，以实现新领域知识（如特定术语）的注入。
作为上述方法的具体应用示例，在智能手机上运行本系统进行长篇小说续写时：当生成日常对话（低熵）时，系统仅使用RNN运行，功耗低；当需要呼应前文伏笔（高熵）时，系统瞬间激活注意力检索历史信息。在基因组分析中，系统以惯性模式快速扫描非编码区，当遇到启动子等高熵关键元件时自动切换至几何模式分析远端调控关系。
本实施例提供了一种基于自适应稀疏门控的序列处理系统，如图2所示，该系统采用热力学门控网络（TGN）架构，运行于包含存储器及处理器（如神经网络处理单元NPU或图形处理器GPU）的计算设备上。在逻辑架构层面，该系统200通过耦合惯性处理模块201、门控判决模块202、几何修正模块203以及自适应融合模块204，实现了对长序列数据的高效处理与动态资源分配。
惯性处理模块201被配置为系统中常驻运行的线性通道。其内部采用循环神经网络（RNN）或状态空间模型（SSM）等具有线性计算复杂度（O(N)）的结构。无论输入数据的复杂程度如何，惯性处理模块始终处于工作状态，负责接收当前时刻的输入向量，并递归地更新一个固定大小的惯性隐状态。该模块的主要功能在于处理序列中常见的局部依赖关系、基础语法结构以及低熵信息，通过模拟物理系统的“惯性”来维持对上下文环境的连续感知，为系统提供低能耗的基础建模能力。
门控判决模块202连接至惯性处理模块，充当系统的自适应判决中枢（即“麦克斯韦妖”）。门控判决模块配置有轻量级的计算单元（如多层感知机），用于实时评估惯性隐状态的特征，检测当前输入对于模型而言的“惊奇度”或预测不确定性。具体而言，该模块计算当前状态的信息熵或几何秩，并据此输出一个标量形式的门控系数。该门控系数作为控制信号，直接决定了系统后续是否需要调用高能耗的计算资源来修正当前的预测结果。
几何修正模块被配置为按需激活的非线性通道。其核心算法采用点积自注意力机制，具有二次计算复杂度（O(N2)），能够执行全局信息的检索与交互。该模块的工作状态严格受控于前述的门控系数：仅当门控判决模块检测到当前的惊奇度超过预设阈值（即处于“关键时刻”）时，几何修正模块才会被启动，通过加载历史键值对缓存来处理长程依赖关系及高熵信息；而在惊奇度低于阈值的绝大多数时间（即“平庸时刻”），该模块处于休眠或物理旁路状态，不执行矩阵运算也不占用显存资源，从而大幅降低系统的平均推理成本。
系统通过自适应融合模块204完成最终的状态输出。该模块连接至惯性处理模块和几何修正模块的输出端，用于根据门控系数对两者的输出特征进行加权求和。通过这种动态融合机制，系统能够在惯性模态与几何模态之间实现平滑切换，既保证了在处理简单模式时的极致效率，又确保了在处理复杂长程关联时的高精度，从而解决了现有技术在计算效率与记忆能力之间的矛盾。
本发明实施例还提供了一种电子设备，如图3所示。该电子设备300包括：处理器301、存储器302、以及存储在存储器上并可在处理器上运行的计算机程序，所述处理器301与存储器302通过总线303连接。所述处理器301执行所述程序时实现上述任一实施例所述的基于自适应稀疏门控的序列处理与优化方法。其中，所述处理器301优选为NPU或GPU，所述存储器302被划分为高速缓存和主存储器以支持前述的分级存储策略。
本发明实施例还提供了一种计算机可读存储介质，其上存储有计算机程序，所述程序被处理器执行时实现上述任一实施例所述的基于自适应稀疏门控的序列处理与优化方法。所述存储介质包括但不限于U盘、移动硬盘、只读存储器（ROM）、随机存取存储器（RAM）、磁碟或者光盘等各种可以存储程序代码的介质。
综上所述，本发明通过创造性地结合线性RNN的惯性特性与Transformer的几何特性，并利用基于熵的自适应门控机制进行动态调度，成功构建了一个既具备无限长程记忆能力，又具备极高推理效率和硬件友好性的序列处理系统。这不仅解决了现有技术中的“不可能三角”难题，还为大模型在端侧设备的普及应用提供了切实可行的技术路径。
在上面详细的说明中，参考了附图，附图形成本文的一部分。在附图中，类似的符号典型地确定类似的部件，除非上下文以其他方式指明。在详细的说明书、附图及权利要求书中所描述的图示说明的实施方案不意味是限制性的。在不脱离本文所呈现的主题的精神或范围下，其他实施方案可以被使用，并且可以作其他改变。
以上所述仅为本发明的优选实施例而已，并不用于限制本发明，对于本领域的技术人员来说，本发明可以有各种更改和变化。凡在本发明的精神和原则之内，所作的任何修改、等同替换、改进等，均应包含在本发明的保护范围之内。


	图1	

图2

图3