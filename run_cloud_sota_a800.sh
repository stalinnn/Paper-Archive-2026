#!/bin/bash

# æ¿€æ´»çŽ¯å¢ƒ
source activate mamba_gold || conda activate mamba_gold

# è®¾ç½® HuggingFace é•œåƒ (å›½å†…é›†ç¾¤åŠ é€Ÿ)
export HF_ENDPOINT=https://hf-mirror.com

# æ˜¾å¡æ•°é‡
GPUS=6
# æ¯ä¸ªGPUçš„Batch Size (A800 80G å¯ä»¥è®¾å¤§ä¸€ç‚¹ï¼Œæ¯”å¦‚ 32 æˆ– 64)
BATCH_SIZE=32

echo "======================================================="
echo "ðŸš€ Starting SOTA Battle on $GPUS x A800 GPUs"
echo "======================================================="

# Ensure output directory exists BEFORE logging starts
mkdir -p result_sota_a800

# 1. Run Mamba (The Challenger)
echo ">>> Running Mamba..."
torchrun --nproc_per_node=$GPUS run_cloud_sota_battle.py \
    --model_type mamba \
    --model_size medium \
    --out_dir result_sota_a800 \
    2>&1 | tee result_sota_a800/mamba.log

# 2. Run Transformer (The Baseline)
echo ">>> Running Transformer..."
torchrun --nproc_per_node=$GPUS run_cloud_sota_battle.py \
    --model_type transformer \
    --model_size medium \
    --out_dir result_sota_a800 \
    2>&1 | tee result_sota_a800/transformer.log

# 3. Run TGN (Our Model)
echo ">>> Running TGN..."
torchrun --nproc_per_node=$GPUS run_cloud_sota_battle.py \
    --model_type tgn \
    --model_size medium \
    --out_dir result_sota_a800 \
    2>&1 | tee result_sota_a800/tgn.log

echo "âœ… All Done! Results are in result_sota_a800/"
